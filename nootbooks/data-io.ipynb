{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Files \n",
    "*[`sc.textFile()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "[u'\"\",\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\",\"Species\"', u'\"1\",5.1,3.5,1.4,0.2,\"setosa\"', u'\"2\",4.9,3,1.4,0.2,\"setosa\"', u'\"3\",4.7,3.2,1.3,0.2,\"setosa\"', u'\"4\",4.6,3.1,1.5,0.2,\"setosa\"']\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"data/iris.csv\")\n",
    "print lines.count()\n",
    "print lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[u'1', u'2', u'3', u'4', u'5']\n"
     ]
    }
   ],
   "source": [
    "lines_zip = sc.textFile(\"data/zips/1.txt.gz\")\n",
    "print lines_zip.count()\n",
    "print lines_zip.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[u'101', u'102', u'103', u'104', u'105']\n"
     ]
    }
   ],
   "source": [
    "lines_zip = sc.textFile(\"data/zips/*.txt.gz\")\n",
    "print lines_zip.count()\n",
    "print lines_zip.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take care of the parameter for `sc.textFile`. By default it uses the `fs.defaultFS` configuration in the hadoop `core-site.xml`.\n",
    "\n",
    "This configuration values differs from one setup to another:\n",
    "- Developement: It is the local file system, relative to the notebook location.\n",
    "- Azure HDInsights: It defaults to an azure storage container, accessible through the [`wasb://` scheme](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-use-blob-storage/).\n",
    "- Amazon EMR: It defaults to `hdfs://`. This means that the data folder needs to be copied to HDFS on your running cluster. On amazon, you can also read data from S3 using the `s3n://` scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.io.IOException: No FileSystem for scheme: s3n\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2584)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2591)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:256)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:203)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1512)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:813)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:374)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f6dad35c5332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Note that this only works inside amazon EMR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlines_zip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"s3n://spark-talk-data/data/zips/*.txt.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mlines_zip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    930\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \"\"\"\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \"\"\"\n\u001b[1;32m--> 923\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    737\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    711\u001b[0m         \"\"\"\n\u001b[0;32m    712\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.io.IOException: No FileSystem for scheme: s3n\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2584)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2591)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:256)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:203)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1512)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:813)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:374)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# Note that this only works inside amazon EMR\n",
    "lines_zip = sc.textFile(\"s3n://spark-talk-data/data/zips/*.txt.gz\")\n",
    "print lines_zip.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "[[u'\"\"', u'\"mpg\"', u'\"cyl\"', u'\"disp\"', u'\"hp\"', u'\"drat\"', u'\"wt\"', u'\"qsec\"', u'\"vs\"', u'\"am\"', u'\"gear\"', u'\"carb\"'], [u'\"Mazda RX4\"', u'21', u'6', u'160', u'110', u'3.9', u'2.62', u'16.46', u'0', u'1', u'4', u'4'], [u'\"Mazda RX4 Wag\"', u'21', u'6', u'160', u'110', u'3.9', u'2.875', u'17.02', u'0', u'1', u'4', u'4']]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"data/mtcars.csv\")\n",
    "print lines.count()\n",
    "print lines.map(lambda x: x.split(\",\")).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better to try spark data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- \"\": string (nullable = true)\n",
      " |-- \"mpg\": float (nullable = true)\n",
      " |-- \"cyl\": float (nullable = true)\n",
      " |-- \"disp\": float (nullable = true)\n",
      " |-- \"hp\": float (nullable = true)\n",
      " |-- \"drat\": float (nullable = true)\n",
      " |-- \"wt\": float (nullable = true)\n",
      " |-- \"qsec\": float (nullable = true)\n",
      " |-- \"vs\": float (nullable = true)\n",
      " |-- \"am\": float (nullable = true)\n",
      " |-- \"gear\": float (nullable = true)\n",
      " |-- \"carb\": float (nullable = true)\n",
      "\n",
      "summary \"mpg\"             \"cyl\"             \"disp\"             \"hp\"              \"drat\"            \"wt\"               \"qsec\"             \"vs\"                \"am\"               \"gear\"             \"carb\"           \n",
      "count   32                32                32                 32                32                32                 32                 32                  32                 32                 32               \n",
      "mean    20.09062498807907 6.1875            230.72187399864197 146.6875          3.59656248241663  3.2172499895095825 17.848749935626984 0.4375              0.40625            3.6875             2.8125           \n",
      "stddev  5.932029706819608 1.757795138803154 121.98678081012338 67.48307079371833 0.526258077920311 0.9630476802463279 1.7588008330430425 0.49607837082461076 0.4911323014219285 0.7261843774138906 1.589762167747113\n",
      "min     10.4              4.0               71.1               52.0              2.76              1.513              14.5               0.0                 0.0                3.0                1.0              \n",
      "max     33.9              8.0               472.0              335.0             4.93              5.424              22.9               1.0                 1.0                5.0                8.0              \n"
     ]
    }
   ],
   "source": [
    "col_names = lines.first().split(\",\")\n",
    "casts = [float]*len(col_names)\n",
    "casts[0] = str\n",
    "\n",
    "types = [FloatType]*len(col_names)\n",
    "types[0] = StringType\n",
    "fields = [StructField(col, col_type(), True) for col, col_type in zip(col_names, types)]\n",
    "schema = StructType(fields)\n",
    "\n",
    "data = lines.map(lambda x: x.split(\",\")).filter(lambda x: x!=col_names)\n",
    "data = data.map(lambda row: map(lambda (cast, cell): cast(cell), zip(casts, row)))\n",
    "\n",
    "mtcars =  sqlContext.createDataFrame(data, schema)\n",
    "mtcars.printSchema()\n",
    "mtcars.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Databases (SQL, Mongo, Elasticsearch, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hadoop input formats are very flexible. These are accessible in spark through [`sc.newAPIHadoopRDD()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.newAPIHadoopRDD)\n",
    "\n",
    "Looking at the function, we are placed back into the Java side of spark.\n",
    "\n",
    "Trying on [MongoDB with spark](https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({u'timeSecond': 1440791303, u'timestamp': 1440791303, u'counter': 2794334, u'__class__': u'org.bson.types.ObjectId', u'processIdentifier': 3375, u'time': 1440791303000L, u'date': datetime.datetime(2015, 8, 28, 19, 48, 23), u'machineIdentifier': 8324479}, {u'High': 24.33, u'Timestamp': u'2009-08-24 09:31', u'Symbol': u'MSFT', u'Volume': 207651, u'Low': 24.28, u'Close': 24.3, u'_id': {u'timeSecond': 1440791303, u'timestamp': 1440791303, u'counter': 2794334, u'__class__': u'org.bson.types.ObjectId', u'processIdentifier': 3375, u'time': 1440791303000L, u'date': datetime.datetime(2015, 8, 28, 19, 48, 23), u'machineIdentifier': 8324479}, u'Open': 24.32, u'Day': 24})]\n",
      "97609\n"
     ]
    }
   ],
   "source": [
    "# for this to run, this requires running pyspark with:\n",
    "# `SPARK_CLASSPATH=/vagrant/meetup/mongo-hadoop-core-1.4.0.jar:/vagrant/meetup/mongo-java-driver-3.0.3.jar pyspark`\n",
    "# or to add the env variable to the processing starting the notebook\n",
    "rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass='com.mongodb.hadoop.MongoInputFormat',\n",
    "    keyClass='org.apache.hadoop.io.Text',\n",
    "    valueClass='org.apache.hadoop.io.MapWritable',\n",
    "    conf={\n",
    "        'mongo.input.uri': 'mongodb://talk:talk@ds035693.mongolab.com:35693/spark-talk.minibars',\n",
    "        'mongo.input.split.create_input_splits': 'false' # this is needed since: the mongodb above is not shared, and there is no access to a cluster manager account. \n",
    "    }\n",
    ")\n",
    "print rdd.take(1)\n",
    "print rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way would be to use the python mongo driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# quick and dirty way to add python packages\n",
    "pymongo_url = \"https://pypi.python.org/packages/2.7/p/pymongo/pymongo-3.0.3-py2.7-macosx-10.10-intel.egg#md5=558e9021691bcf1051946d1f9e122d59\"\n",
    "pymongo_dest = \"pymongo.egg\"\n",
    "import os\n",
    "os.system(\"wget %s -O %s\" %(pymongo_url, pymongo_dest))\n",
    "sc.addPyFile(pymongo_dest) \n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'Volume': 207651, u'Timestamp': u'2009-08-24 09:31', u'Symbol': u'MSFT', u'High': 24.33, u'Low': 24.28, u'Close': 24.3, u'_id': ObjectId('55e0bb077f057f0d2f2aa35e'), u'Open': 24.32, u'Day': 24}, {u'Volume': 683713, u'Timestamp': u'2009-08-24 09:30', u'Symbol': u'MSFT', u'High': 24.42, u'Low': 24.31, u'Close': 24.31, u'_id': ObjectId('55e0bb087f057f0d2f2aa35f'), u'Open': 24.41, u'Day': 24}]\n",
      "97609 97600\n"
     ]
    }
   ],
   "source": [
    "def get_collection():\n",
    "    client = pymongo.MongoClient('mongodb://talk:talk@ds035693.mongolab.com:35693/spark-talk')\n",
    "    db = client['spark-talk']\n",
    "    return db.minibars\n",
    "\n",
    "def get_split(skip, limit = 100):\n",
    "    collection = get_collection()\n",
    "    return collection.find().limit(limit).skip(skip)\n",
    "    \n",
    "count = get_collection().count()\n",
    "\n",
    "splits_rdd = sc.parallelize(xrange(count/100))\n",
    "data = splits_rdd.flatMap(get_split).cache()\n",
    "print data.take(2)\n",
    "print count, data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Day: long (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Symbol: string (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Volume: long (nullable = true)\n",
      " |-- _id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_id_to_string(row):\n",
    "    row[\"_id\"] = str(row[\"_id\"])\n",
    "    return row\n",
    "data = data.map(convert_id_to_string)\n",
    "minibars_df = sqlContext.createDataFrame(data.map(lambda row: pyspark.sql.Row(**row)))\n",
    "minibars_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`rdd.saveAsTextFile()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.saveAsTextFile) Here you also have to take care of the file system scheme used in the paramter.\n",
    "- [`rdd.saveAsSequenceFile()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.saveAsSequenceFile) This serializes an RDD to a binary file.\n",
    "- For external databases, the same concepts can be used as in reading from external databases.\n",
    "- [`rdd.saveAsPickleFile()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.saveAsPickleFile) This is another way of serializing data to a binary file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
